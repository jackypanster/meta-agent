#!/usr/bin/env python3
"""
Qwen-Agent MVP - ÁÆÄÊ¥ÅÁõ¥ËßÇÂÆûÁé∞

Âü∫‰∫éÂÆòÊñπQwen-AgentÊ°ÜÊû∂ÁöÑÊúÄÁÆÄÊ¥ÅÂÆûÁé∞Ôºö
- Áõ¥Êé•‰ΩøÁî®AssistantÁ±ª
- Ê®°ÂùóÂåñÂ∑•ÂÖ∑Á≥ªÁªü
- ÁÆÄÂçïÁöÑÂÜÖÂ≠òÁÆ°ÁêÜ
- Áõ¥ËßÇÁöÑCLIÁïåÈù¢
- ‰ΩøÁî®ÊúÄÊñ∞DeepSeek-R1-0528Êé®ÁêÜÊ®°Âûã
"""

import time
import requests
from typing import Dict, List, Any, NoReturn

# Qwen-Agent imports
from qwen_agent.agents import Assistant
from qwen_agent.utils.output_beautify import typewriter_print

# ÂØºÂÖ•Â∑•ÂÖ∑Á±ª - ‰ΩøÁî®ÁªùÂØπÂØºÂÖ•
from src.tools.qwen_tools.memory_tools import get_memory_store

# ÂØºÂÖ•ÈÖçÁΩÆÁÆ°ÁêÜ
from src.config.settings import get_config, ConfigError
from src.config.mcp_config import get_mcp_config_loader
from src.config.prompt_manager import PromptManager, PromptManagerError

# ÂØºÂÖ•UIÂ∏ÆÂä©ÂáΩÊï∞
from src.ui import show_welcome, show_help, show_memory, clear_screen

# ÂÖ®Â±ÄPromptManagerÂÆû‰æã
prompt_manager = None


class APIConnectionError(Exception):
    """APIËøûÊé•ÈîôËØØ"""
    pass


class ModelConfigError(Exception):
    """Ê®°ÂûãÈÖçÁΩÆÈîôËØØ"""
    pass


class MCPConfigError(Exception):
    """MCPÈÖçÁΩÆÈîôËØØ"""
    pass


def initialize_prompt_manager() -> PromptManager:
    """ÂàùÂßãÂåñPromptManager - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    Returns:
        PromptManagerÂÆû‰æã
        
    Raises:
        PromptManagerError: ÊèêÁ§∫ËØçÈÖçÁΩÆÂä†ËΩΩÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
    """
    global prompt_manager
    
    prompt_manager = PromptManager("config/prompts")
    print("‚úì ÊèêÁ§∫ËØçÈÖçÁΩÆÂä†ËΩΩÊàêÂäü")
    return prompt_manager


def get_prompt(prompt_key: str, variables: Dict[str, Any] = None) -> str:
    """Ëé∑ÂèñÊèêÁ§∫ËØçÔºåÈÖçÁΩÆÁº∫Â§±Êó∂Âø´ÈÄüÂ§±Ë¥•
    
    Args:
        prompt_key: ÊèêÁ§∫ËØçÈîÆ
        variables: ÂèòÈáèÊõøÊç¢Â≠óÂÖ∏
        
    Returns:
        ÊèêÁ§∫ËØçÂÜÖÂÆπ
        
    Raises:
        PromptManagerError: ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®ÊàñÈÖçÁΩÆÈîôËØØÊó∂Á´ãÂç≥Â§±Ë¥•
    """
    global prompt_manager
    
    if not prompt_manager:
        raise PromptManagerError(f"PromptManagerÊú™ÂàùÂßãÂåñÔºÅÊó†Ê≥ïËé∑ÂèñÊèêÁ§∫ËØç: {prompt_key}")
    
    return prompt_manager.get_prompt(prompt_key, variables)


def setup_mcp_servers() -> Dict[str, Any]:
    """ËÆæÁΩÆMCPÊúçÂä°Âô®ÈÖçÁΩÆ - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    ‰ªéÈÖçÁΩÆÊñá‰ª∂Âä®ÊÄÅÂä†ËΩΩÂêØÁî®ÁöÑMCPÊúçÂä°Âô®
    
    Returns:
        MCPÊúçÂä°Âô®ÈÖçÁΩÆÂ≠óÂÖ∏ÔºåÁ¨¶ÂêàQwen-AgentÊ†ºÂºè
        
    Raises:
        MCPConfigError: MCPÈÖçÁΩÆÂä†ËΩΩÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
    """
    # Ëé∑ÂèñMCPÈÖçÁΩÆÂä†ËΩΩÂô®
    config_loader = get_mcp_config_loader()
    
    # Ëé∑ÂèñÂêØÁî®ÁöÑÊúçÂä°Âô®
    enabled_servers = config_loader.get_enabled_servers()
    
    if not enabled_servers:
        raise MCPConfigError("‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïÂêØÁî®ÁöÑMCPÊúçÂä°Âô®")
    
    # ÊûÑÂª∫Qwen-AgentÊ†ºÂºèÁöÑMCPÈÖçÁΩÆ
    mcp_servers = {}
    
    # This function intentionally only uses a subset of the MCP server
    # configuration (command, args, env) for the agent's tool setup.
    # Other fields in mcp_servers.json might be for the server
    # processes themselves or other administrative purposes.
    for server_name in enabled_servers:
        server_config = config_loader.get_server_config(server_name)
        if not server_config:
            raise MCPConfigError(f"‚ùå ÊúçÂä°Âô® '{server_name}' ÈÖçÁΩÆ‰∏çÂ≠òÂú®")
        
        # ËΩ¨Êç¢‰∏∫Qwen-AgentÊúüÊúõÁöÑÊ†ºÂºè
        qwen_config = {
            'command': server_config['command'],
            'args': server_config['args']
        }
        
        # Ê∑ªÂä†ÁéØÂ¢ÉÂèòÈáèÔºàÂ¶ÇÊûúÊúâÔºâ
        if 'env' in server_config:
            qwen_config['env'] = server_config['env']
        
        mcp_servers[server_name] = qwen_config
        
        # ÊòæÁ§∫Âä†ËΩΩÁöÑÊúçÂä°Âô®‰ø°ÊÅØ
        category = server_config.get('category', 'Êú™ÂàÜÁ±ª')
        print(f"‚úì Âä†ËΩΩMCPÊúçÂä°Âô®: {server_name} (ÂàÜÁ±ª: {category})")
    
    print(f"üì° ÊàêÂäüÂä†ËΩΩ {len(mcp_servers)} ‰∏™MCPÊúçÂä°Âô®")
    return mcp_servers


def create_llm_config() -> Dict[str, Any]:
    """ÂàõÂª∫LLMÈÖçÁΩÆ - Ê†πÊçÆÊ®°ÂûãÂêçÁß∞Ëá™Âä®Ê£ÄÊµãÊèê‰æõÂïÜÔºåÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    ÊîØÊåÅÁöÑÊ®°Âûã:
    - deepseek-chat: DeepSeek V3 Á®≥ÂÆöÊ®°Âûã
    - deepseek-reasoner: DeepSeek R1 Êé®ÁêÜÊ®°Âûã  
    - qwen3-32b: Êú¨Âú∞ÈÉ®ÁΩ≤ÁöÑQwen3-32BÊ®°Âûã
    
    Returns:
        LLMÈÖçÁΩÆÂ≠óÂÖ∏
        
    Raises:
        ConfigError: ÈÖçÁΩÆÂä†ËΩΩÊàñAPIÂØÜÈí•È™åËØÅÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
        ModelConfigError: Ê®°ÂûãÈÖçÁΩÆÈîôËØØÊó∂Á´ãÂç≥ÊäõÂá∫
    """
    
    config = get_config()
    
    # Ëé∑ÂèñÊ®°ÂûãÂêçÁß∞ÈÖçÁΩÆ - ÂøÖÈúÄÂ≠óÊÆµ
    model_name = config.require('MODEL_NAME').lower()
    
    if model_name in ['deepseek-chat', 'deepseek-reasoner']:
        return _create_deepseek_config(config, model_name)
    elif model_name == 'qwen3-32b':
        return _create_qwen3_config(config)
    else:
        raise ModelConfigError(
            f"‚ùå ‰∏çÊîØÊåÅÁöÑÊ®°Âûã: {model_name}\n"
            f"ÊîØÊåÅÁöÑÊ®°Âûã: deepseek-chat, deepseek-reasoner, qwen3-32b\n"
            f"ËØ∑Âú®.envÊñá‰ª∂‰∏≠ËÆæÁΩÆ MODEL_NAME=deepseek-chat ÊàñÂÖ∂‰ªñÊîØÊåÅÁöÑÊ®°Âûã"
        )


def _create_deepseek_config(config, model_name: str) -> Dict[str, Any]:
    """ÂàõÂª∫DeepSeekÈÖçÁΩÆ
    
    Args:
        config: ÈÖçÁΩÆÂÆû‰æã
        model_name: Ê®°ÂûãÂêçÁß∞ (deepseek-chat Êàñ deepseek-reasoner)
        
    Returns:
        DeepSeek LLMÈÖçÁΩÆÂ≠óÂÖ∏
        
    Raises:
        ConfigError: DeepSeekÈÖçÁΩÆÈîôËØØÊó∂Á´ãÂç≥ÊäõÂá∫
    """
    # Ê£ÄÊü•DeepSeek APIÂØÜÈí• - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    api_key = config.require('DEEPSEEK_API_KEY')
    
    print("üîç Ê£ÄÊµãÂà∞DeepSeek APIÂØÜÈí•")
    
    base_url = 'https://api.deepseek.com/v1'
    
    if model_name == 'deepseek-reasoner':
        model = 'deepseek-reasoner'  # R1-0528Êé®ÁêÜÊ®°Âûã
        display_name = "DeepSeek R1-0528 Êé®ÁêÜÊ®°Âûã"
    else:  # deepseek-chat
        model = 'deepseek-chat'  # V3-0324 Á®≥ÂÆöÊ®°Âûã
        display_name = "DeepSeek V3 Á®≥ÂÆöÊ®°Âûã"
    
    print(f"‚ö° ‰ΩøÁî®{display_name}")
    
    return {
        'model': model,
        'model_server': base_url,
        'api_key': api_key,
        'generate_cfg': {
            'top_p': 0.8,
            'max_tokens': 2000,
            'temperature': 0.7
        }
    }


def _create_qwen3_config(config) -> Dict[str, Any]:
    """ÂàõÂª∫Qwen3-32BÈÖçÁΩÆ - ÊîØÊåÅÊÄùËÄÉÊ®°ÂºèÈÖçÁΩÆ
    
    Args:
        config: ÈÖçÁΩÆÂÆû‰æã
        
    Returns:
        Qwen3 LLMÈÖçÁΩÆÂ≠óÂÖ∏ÔºåÂåÖÂê´vLLM/SGLangÂÖºÂÆπÈÖçÁΩÆ
        
    Raises:
        ConfigError: Qwen3ÈÖçÁΩÆÈîôËØØÊó∂Á´ãÂç≥ÊäõÂá∫
    """
    # Ê£ÄÊü•Qwen3 APIÂØÜÈí• - ÈÄöÂ∏∏ÊòØ"EMPTY"
    api_key = config.require('QWEN3_API_KEY')
    
    # Ê£ÄÊü•Qwen3ÊúçÂä°Âô®Âú∞ÂùÄ
    model_server = config.require('QWEN3_MODEL_SERVER')
    
    # Ê£ÄÊü•ÊòØÂê¶ÂêØÁî®ÊÄùËÄÉÊ®°Âºè - ÂèØÈÄâÈÖçÁΩÆÔºåÈªòËÆ§‰∏∫false
    enable_thinking = config.get_bool_optional('QWEN3_ENABLE_THINKING', default=False)
    
    print("üîç Ê£ÄÊµãÂà∞Qwen3-32BÈÖçÁΩÆ")
    print(f"üì° ÊúçÂä°Âô®Âú∞ÂùÄ: {model_server}")
    print(f"üß† ÊÄùËÄÉÊ®°Âºè: {'ÂêØÁî®' if enable_thinking else 'Á¶ÅÁî®'}")
    
    # Qwen3-32BÊ®°ÂûãÂêçÁß∞ÔºàÊ†πÊçÆÂÆòÊñπÁ§∫‰æãÔºâ
    model = 'Qwen/Qwen3-32B'
    model_name = "Qwen3-32B Êú¨Âú∞ÈÉ®ÁΩ≤Ê®°Âûã"
    
    print(f"‚ö° ‰ΩøÁî®{model_name}")
    
    # ÊûÑÂª∫generate_cfgÈÖçÁΩÆ
    generate_cfg = {
        'top_p': 0.8,
        'max_tokens': 2000,
        'temperature': 0.7,
        'extra_body': {
            'chat_template_kwargs': {
                'enable_thinking': enable_thinking
            }
        }
    }
    
    return {
        'model': model,
        'model_server': model_server,
        'api_key': api_key,
        'generate_cfg': generate_cfg
    }


def create_tools_list() -> List[Any]:
    """ÂàõÂª∫Â∑•ÂÖ∑ÂàóË°® - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    Âä®ÊÄÅÊûÑÂª∫ÂåÖÂê´MCPÊúçÂä°Âô®ÁöÑÂ∑•ÂÖ∑ÂàóË°®
    
    Returns:
        Â∑•ÂÖ∑ÂàóË°®ÔºåÂåÖÂê´Ëá™ÂÆö‰πâÂ∑•ÂÖ∑ÂíåMCPÊúçÂä°Âô®ÈÖçÁΩÆ
        
    Raises:
        MCPConfigError: MCPÈÖçÁΩÆÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
    """
    # ËÆæÁΩÆMCPÊúçÂä°Âô®
    mcp_servers = setup_mcp_servers()
    
    # ÊûÑÂª∫Â∑•ÂÖ∑ÂàóË°®
    # These tools are explicitly listed to ensure their availability to the agent.
    # Relying on potential auto-discovery mechanisms of the Qwen framework
    # is not currently implemented or confirmed for this project.
    tools = [
        'custom_save_info', 
        'custom_recall_info', 
        'custom_math_calc',
        {
            'mcpServers': mcp_servers  # ‰ΩøÁî®Âä®ÊÄÅÂä†ËΩΩÁöÑMCPÈÖçÁΩÆ
        },
        'code_interpreter',  # ÂÜÖÁΩÆ‰ª£Á†ÅËß£ÈáäÂô®Â∑•ÂÖ∑
    ]
    
    return tools


def main() -> NoReturn:
    """‰∏ªÂáΩÊï∞ - ‰∏ìÊ≥®‰∫éÁ®ãÂ∫èÊµÅÁ®ãÊéßÂà∂ÔºåÂ§±Ë¥•Êó∂Á´ãÂç≥Â¥©Ê∫É
    
    Raises:
        PromptManagerError: ÊèêÁ§∫ËØçÈÖçÁΩÆÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
        ConfigError: ÈÖçÁΩÆÂä†ËΩΩÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
        MCPConfigError: MCPÈÖçÁΩÆÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
        Exception: ‰ªª‰ΩïÂÖ∂‰ªñÂºÇÂ∏∏ÈÉΩ‰ºöÁ´ãÂç≥‰º†Êí≠ÂØºËá¥Á®ãÂ∫èÂ¥©Ê∫É
    """
    # 1. ÂàùÂßãÂåñÊèêÁ§∫ËØçÁÆ°ÁêÜÂô®
    initialize_prompt_manager()
    
    # 2. ÊòæÁ§∫Ê¨¢ËøéÁïåÈù¢
    show_welcome()
    
    # 3. ÂàõÂª∫Agent
    ai_loading_msg = get_prompt("ai_loading")
    print(f"\n{ai_loading_msg}")
    
    llm_cfg = create_llm_config()
    
    # 4. ËÆæÁΩÆMCPÊúçÂä°Âô®ÂíåÂ∑•ÂÖ∑
    mcp_loading_msg = get_prompt("mcp_loading")
    print(f"\n{mcp_loading_msg}")
    
    tools = create_tools_list()
    
    # Ëé∑ÂèñÁ≥ªÁªüÊèêÁ§∫ËØç - ‰ªéÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩ
    system_message = get_prompt("system_base")

    # ÂàõÂª∫Agent - ÂèÇËÄÉÂÆòÊñπQwen3Á§∫‰æã
    # Ëé∑ÂèñAgentÈÖçÁΩÆ
    agent_name = get_prompt("agent_name")
    agent_description = get_prompt("agent_description")
    
    agent = Assistant(
        llm=llm_cfg,
        system_message=system_message,
        function_list=tools,
        name=agent_name,
        description=agent_description
    )
    
    ai_success_msg = get_prompt("ai_success")
    print(ai_success_msg)
    
    # 5. ÂØπËØùÂæ™ÁéØ - ‰ªª‰ΩïÂºÇÂ∏∏ÈÉΩÁ´ãÂç≥Â¥©Ê∫É
    messages = []
    memory_store = get_memory_store()
    config = get_config()
    
    # Ê†πÊçÆÊ®°ÂûãÂêçÁß∞Á°ÆÂÆöÊòæÁ§∫ÂêçÁß∞
    model_name = config.require('MODEL_NAME').lower()
    if model_name == 'deepseek-chat':
        model_display = "DeepSeek-V3Á®≥ÂÆöÊ®°Âûã"
    elif model_name == 'deepseek-reasoner':
        model_display = "DeepSeek-R1Êé®ÁêÜÊ®°Âûã"
    elif model_name == 'qwen3-32b':
        enable_thinking = config.get_bool_optional('QWEN3_ENABLE_THINKING', default=False)
        thinking_suffix = "(ÊÄùËÄÉÊ®°Âºè)" if enable_thinking else "(Ê†áÂáÜÊ®°Âºè)"
        model_display = f"Qwen3-32BÊú¨Âú∞ÈÉ®ÁΩ≤Ê®°Âûã{thinking_suffix}"
    else:
        model_display = f"Êú™Áü•Ê®°Âûã({model_name})"
    
    conversation_start_msg = get_prompt(
        "conversation_start",
        {"model_display": model_display}
    )
    print(f"\n{conversation_start_msg}\n")
    
    while True:
        # Ëé∑ÂèñÁî®Êà∑ËæìÂÖ• - Âè™Â§ÑÁêÜÁî®Êà∑‰∏≠Êñ≠
        user_input = input("ÊÇ®: ").strip()
        
        # Â§ÑÁêÜÁâπÊÆäÂëΩ‰ª§
        if user_input.lower() in ['quit', 'exit', 'q', 'ÈÄÄÂá∫']:
            goodbye_msg = get_prompt("goodbye_message")
            print(goodbye_msg)
            break
        elif user_input.lower() in ['help', 'h', 'Â∏ÆÂä©']:
            show_help()
            continue
        elif user_input.lower() in ['clear', 'cls', 'Ê∏ÖÂ±è']:
            clear_screen()
            continue
        elif user_input.lower() in ['memory', 'mem', 'ËÆ∞ÂøÜ']:
            show_memory()
            continue
        elif not user_input:
            continue
        
        # Ê∑ªÂä†Áî®Êà∑Ê∂àÊÅØÂà∞ÂéÜÂè≤
        messages.append({'role': 'user', 'content': user_input})
        
        # ÊòæÁ§∫AIÂõûÂ§ç
        ai_response_prefix = get_prompt("ai_response_prefix")
        print(f"\n{ai_response_prefix}", end='', flush=True)
        
        # Ë∞ÉÁî®AgentÂπ∂ÊµÅÂºèÊòæÁ§∫.
        # Ê≥®ÊÑè: ‰ªª‰ΩïÊù•Ëá™ agent.run() ÁöÑÂºÇÂ∏∏ (‰æãÂ¶ÇÂ∑•ÂÖ∑ÊâßË°åÈîôËØØ„ÄÅLLM APIÈîôËØØÁ≠â)
        # Âú®ËøôÈáåÈÉΩÊ≤°ÊúâË¢´ÊçïËé∑ÔºåËøôÊòØ‰∏∫‰∫ÜÁ°Æ‰øùÂΩìÂâç‰∫§‰∫íËΩÆÊ¨°ÁöÑ fail-fast Ë°å‰∏∫„ÄÇ
        # ËøôÁßçËÆæËÆ°Á¨¶ÂêàÈ°πÁõÆÁ´ãÂç≥Êö¥Èú≤ÈóÆÈ¢òÁöÑÂéüÂàô„ÄÇ
        response_text = ""
        response_messages = agent.run(messages=messages)
        
        for response in response_messages:
            response_text = typewriter_print(response, response_text)
        
        # Ê∏ÖÁêÜÂπ∂Ê∑ªÂä†ÂìçÂ∫îÂà∞ÂéÜÂè≤ - ÁâπÂà´Â§ÑÁêÜR1Ê®°ÂûãÁöÑreasoning_content
        clean_messages = []
        for msg in response_messages:
            if isinstance(msg, dict):
                # ÂàõÂª∫Ê∏ÖÁêÜÂêéÁöÑÊ∂àÊÅØÂâØÊú¨ÔºåÁßªÈô§reasoning_content
                clean_msg = {k: v for k, v in msg.items() if k != 'reasoning_content'}
                clean_messages.append(clean_msg)
            else:
                clean_messages.append(msg)
        
        messages.extend(clean_messages)
        
        # ‰øùÂ≠òÂØπËØùÂà∞ÁÆÄÂçïÂéÜÂè≤ËÆ∞ÂΩï
        memory_store['history'].append({
            'user': user_input,
            'assistant': response_text,
            'timestamp': time.time()
        })
        
        # ‰øùÊåÅÂéÜÂè≤ËÆ∞ÂΩï‰∏çË∂ÖËøá50Êù°
        if len(memory_store['history']) > 50:
            memory_store['history'] = memory_store['history'][-50:]
        
        print()  # Êç¢Ë°å


if __name__ == "__main__":
    main() 