#!/usr/bin/env python3
"""
Qwen-Agent MVP - ÁÆÄÊ¥ÅÁõ¥ËßÇÂÆûÁé∞

Âü∫‰∫éÂÆòÊñπQwen-AgentÊ°ÜÊû∂ÁöÑÊúÄÁÆÄÊ¥ÅÂÆûÁé∞Ôºö
- Áõ¥Êé•‰ΩøÁî®AssistantÁ±ª
- Ê®°ÂùóÂåñÂ∑•ÂÖ∑Á≥ªÁªü
- ÁÆÄÂçïÁöÑÂÜÖÂ≠òÁÆ°ÁêÜ
- Áõ¥ËßÇÁöÑCLIÁïåÈù¢
- ‰ΩøÁî®ÊúÄÊñ∞DeepSeek-R1-0528Êé®ÁêÜÊ®°Âûã
"""

import time
import requests
from typing import Dict, List, Any, NoReturn

# Qwen-Agent imports
from qwen_agent.agents import Assistant
from qwen_agent.utils.output_beautify import typewriter_print

# ÂØºÂÖ•Â∑•ÂÖ∑Á±ª - ‰ΩøÁî®ÁªùÂØπÂØºÂÖ•
from src.tools.qwen_tools.memory_tools import get_memory_store

# ÂØºÂÖ•ÈÖçÁΩÆÁÆ°ÁêÜ
from src.config.settings import get_config, ConfigError
from src.config.mcp_config import get_mcp_config_loader
from src.config.prompt_manager import PromptManager, PromptManagerError

# ÂØºÂÖ•UIÂ∏ÆÂä©ÂáΩÊï∞
from src.ui import show_welcome, show_help, show_memory, clear_screen

# ÂÖ®Â±ÄPromptManagerÂÆû‰æã
prompt_manager = None


class APIConnectionError(Exception):
    """APIËøûÊé•ÈîôËØØ"""
    pass


class ModelConfigError(Exception):
    """Ê®°ÂûãÈÖçÁΩÆÈîôËØØ"""
    pass


class MCPConfigError(Exception):
    """MCPÈÖçÁΩÆÈîôËØØ"""
    pass


def initialize_prompt_manager() -> PromptManager:
    """ÂàùÂßãÂåñPromptManager - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    Returns:
        PromptManagerÂÆû‰æã
        
    Raises:
        PromptManagerError: ÊèêÁ§∫ËØçÈÖçÁΩÆÂä†ËΩΩÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
    """
    global prompt_manager
    
    prompt_manager = PromptManager("config/prompts")
    print("‚úì ÊèêÁ§∫ËØçÈÖçÁΩÆÂä†ËΩΩÊàêÂäü")
    return prompt_manager


def get_prompt(prompt_key: str, variables: Dict[str, Any] = None) -> str:
    """Ëé∑ÂèñÊèêÁ§∫ËØçÔºåÈÖçÁΩÆÁº∫Â§±Êó∂Âø´ÈÄüÂ§±Ë¥•
    
    Args:
        prompt_key: ÊèêÁ§∫ËØçÈîÆ
        variables: ÂèòÈáèÊõøÊç¢Â≠óÂÖ∏
        
    Returns:
        ÊèêÁ§∫ËØçÂÜÖÂÆπ
        
    Raises:
        PromptManagerError: ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®ÊàñÈÖçÁΩÆÈîôËØØÊó∂Á´ãÂç≥Â§±Ë¥•
    """
    global prompt_manager
    
    if not prompt_manager:
        raise PromptManagerError(f"PromptManagerÊú™ÂàùÂßãÂåñÔºÅÊó†Ê≥ïËé∑ÂèñÊèêÁ§∫ËØç: {prompt_key}")
    
    return prompt_manager.get_prompt(prompt_key, variables)


def setup_mcp_servers() -> Dict[str, Any]:
    """
    Loads and formats enabled MCP server configurations for use as agent tools.
    
    Dynamically retrieves enabled MCP servers from configuration, extracting only the fields required by the agent (command, args, and optionally env) and returning them in the format expected by Qwen-Agent. Raises MCPConfigError if no servers are enabled or if a server's configuration is missing.
    
    Returns:
        A dictionary mapping server names to their Qwen-Agent-compatible configuration.
    
    Raises:
        MCPConfigError: If no enabled servers are found or a server configuration is missing.
    """
    # Ëé∑ÂèñMCPÈÖçÁΩÆÂä†ËΩΩÂô®
    config_loader = get_mcp_config_loader()
    
    # Ëé∑ÂèñÂêØÁî®ÁöÑÊúçÂä°Âô®
    enabled_servers = config_loader.get_enabled_servers()
    
    if not enabled_servers:
        raise MCPConfigError("‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïÂêØÁî®ÁöÑMCPÊúçÂä°Âô®")
    
    # ÊûÑÂª∫Qwen-AgentÊ†ºÂºèÁöÑMCPÈÖçÁΩÆ
    mcp_servers = {}
    
    # This function intentionally only uses a subset of the MCP server
    # configuration (command, args, env) for the agent's tool setup.
    # Other fields in mcp_servers.json might be for the server
    # processes themselves or other administrative purposes.
    for server_name in enabled_servers:
        server_config = config_loader.get_server_config(server_name)
        if not server_config:
            raise MCPConfigError(f"‚ùå ÊúçÂä°Âô® '{server_name}' ÈÖçÁΩÆ‰∏çÂ≠òÂú®")
        
        # ËΩ¨Êç¢‰∏∫Qwen-AgentÊúüÊúõÁöÑÊ†ºÂºè
        qwen_config = {
            'command': server_config['command'],
            'args': server_config['args']
        }
        
        # Ê∑ªÂä†ÁéØÂ¢ÉÂèòÈáèÔºàÂ¶ÇÊûúÊúâÔºâ
        if 'env' in server_config:
            qwen_config['env'] = server_config['env']
        
        mcp_servers[server_name] = qwen_config
        
        # ÊòæÁ§∫Âä†ËΩΩÁöÑÊúçÂä°Âô®‰ø°ÊÅØ
        category = server_config.get('category', 'Êú™ÂàÜÁ±ª')
        print(f"‚úì Âä†ËΩΩMCPÊúçÂä°Âô®: {server_name} (ÂàÜÁ±ª: {category})")
    
    print(f"üì° ÊàêÂäüÂä†ËΩΩ {len(mcp_servers)} ‰∏™MCPÊúçÂä°Âô®")
    return mcp_servers


def create_llm_config() -> Dict[str, Any]:
    """ÂàõÂª∫LLMÈÖçÁΩÆ - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    
    Returns:
        LLMÈÖçÁΩÆÂ≠óÂÖ∏
        
    Raises:
        ConfigError: ÈÖçÁΩÆÂä†ËΩΩÊàñAPIÂØÜÈí•È™åËØÅÂ§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫
    """
    
    config = get_config()
    
    # Ê£ÄÊü•ÊòØÂê¶Ë¶Å‰ΩøÁî®R1Êé®ÁêÜÊ®°Âûã
    use_r1 = config.get_bool('USE_DEEPSEEK_R1')
    
    # Ê£ÄÊü•DeepSeek APIÂØÜÈí• - Â§±Ë¥•Êó∂Á´ãÂç≥ÊäõÂá∫ÂºÇÂ∏∏
    api_key = config.require('DEEPSEEK_API_KEY')
    
    print("üîç Ê£ÄÊµãÂà∞DeepSeek APIÂØÜÈí•")
    
    base_url = 'https://api.deepseek.com/v1'
    
    if use_r1:
        model = 'deepseek-reasoner'  # R1-0528Êé®ÁêÜÊ®°Âûã
        model_name = "DeepSeek R1-0528 Êé®ÁêÜÊ®°Âûã"
    else:
        model = 'deepseek-chat'  # V3-0324 Á®≥ÂÆöÊ®°Âûã
        model_name = "DeepSeek V3 Á®≥ÂÆöÊ®°Âûã"
    
    # ‰∏çËøõË°åËøûÊé•ÊµãËØï - ‰ªª‰ΩïËøûÊé•ÈóÆÈ¢òÈÉΩÂú®ÂÆûÈôÖË∞ÉÁî®Êó∂Á´ãÂç≥Êö¥Èú≤
    print(f"‚ö° ‰ΩøÁî®{model_name}")
    
    return {
        'model': model,
        'model_server': base_url,
        'api_key': api_key,
        'generate_cfg': {
            'top_p': 0.8,
            'max_tokens': 2000,
            'temperature': 0.7
        }
    }


def create_tools_list() -> List[Any]:
    """
    Constructs and returns the list of tools available to the conversational agent.
    
    The tool list includes custom tools, dynamically loaded MCP server configurations, and a built-in code interpreter. Raises an exception if MCP server setup fails.
    
    Returns:
        A list containing custom tool identifiers, a dictionary of MCP server configurations, and the code interpreter tool.
    
    Raises:
        MCPConfigError: If MCP server configuration fails.
    """
    # ËÆæÁΩÆMCPÊúçÂä°Âô®
    mcp_servers = setup_mcp_servers()
    
    # ÊûÑÂª∫Â∑•ÂÖ∑ÂàóË°®
    # These tools are explicitly listed to ensure their availability to the agent.
    # Relying on potential auto-discovery mechanisms of the Qwen framework
    # is not currently implemented or confirmed for this project.
    tools = [
        'custom_save_info', 
        'custom_recall_info', 
        'custom_math_calc',
        {
            'mcpServers': mcp_servers  # ‰ΩøÁî®Âä®ÊÄÅÂä†ËΩΩÁöÑMCPÈÖçÁΩÆ
        },
        'code_interpreter',  # ÂÜÖÁΩÆ‰ª£Á†ÅËß£ÈáäÂô®Â∑•ÂÖ∑
    ]
    
    return tools


def main() -> NoReturn:
    """
    Runs the main program loop for the conversational AI CLI, handling initialization, user interaction, and conversation management.
    
    Initializes the prompt manager, displays welcome and loading messages, configures the LLM and tools, and creates the conversational agent. Enters an interactive loop to process user input, handle special commands (help, clear, memory, exit), manage conversation history, and stream AI responses. Conversation pairs are stored in a simple memory store, retaining the latest 50 exchanges. Any configuration or runtime errors propagate immediately, causing the program to terminate.
    """
    # 1. ÂàùÂßãÂåñÊèêÁ§∫ËØçÁÆ°ÁêÜÂô®
    initialize_prompt_manager()
    
    # 2. ÊòæÁ§∫Ê¨¢ËøéÁïåÈù¢
    show_welcome()
    
    # 3. ÂàõÂª∫Agent
    ai_loading_msg = get_prompt("ai_loading")
    print(f"\n{ai_loading_msg}")
    
    llm_cfg = create_llm_config()
    
    # 4. ËÆæÁΩÆMCPÊúçÂä°Âô®ÂíåÂ∑•ÂÖ∑
    mcp_loading_msg = get_prompt("mcp_loading")
    print(f"\n{mcp_loading_msg}")
    
    tools = create_tools_list()
    
    # Ëé∑ÂèñÁ≥ªÁªüÊèêÁ§∫ËØç - ‰ªéÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩ
    system_message = get_prompt("system_base")

    # ÂàõÂª∫Agent - ÂèÇËÄÉÂÆòÊñπQwen3Á§∫‰æã
    # Ëé∑ÂèñAgentÈÖçÁΩÆ
    agent_name = get_prompt("agent_name")
    agent_description = get_prompt("agent_description")
    
    agent = Assistant(
        llm=llm_cfg,
        system_message=system_message,
        function_list=tools,
        name=agent_name,
        description=agent_description
    )
    
    ai_success_msg = get_prompt("ai_success")
    print(ai_success_msg)
    
    # 5. ÂØπËØùÂæ™ÁéØ - ‰ªª‰ΩïÂºÇÂ∏∏ÈÉΩÁ´ãÂç≥Â¥©Ê∫É
    messages = []
    memory_store = get_memory_store()
    config = get_config()
    use_r1 = config.get_bool('USE_DEEPSEEK_R1')
    model_display = "DeepSeek-R1Êé®ÁêÜÊ®°Âûã" if use_r1 else "DeepSeek-V3Á®≥ÂÆöÊ®°Âûã"
    
    conversation_start_msg = get_prompt(
        "conversation_start",
        {"model_display": model_display}
    )
    print(f"\n{conversation_start_msg}\n")
    
    while True:
        # Ëé∑ÂèñÁî®Êà∑ËæìÂÖ• - Âè™Â§ÑÁêÜÁî®Êà∑‰∏≠Êñ≠
        user_input = input("ÊÇ®: ").strip()
        
        # Â§ÑÁêÜÁâπÊÆäÂëΩ‰ª§
        if user_input.lower() in ['quit', 'exit', 'q', 'ÈÄÄÂá∫']:
            goodbye_msg = get_prompt("goodbye_message")
            print(goodbye_msg)
            break
        elif user_input.lower() in ['help', 'h', 'Â∏ÆÂä©']:
            show_help()
            continue
        elif user_input.lower() in ['clear', 'cls', 'Ê∏ÖÂ±è']:
            clear_screen()
            continue
        elif user_input.lower() in ['memory', 'mem', 'ËÆ∞ÂøÜ']:
            show_memory()
            continue
        elif not user_input:
            continue
        
        # Ê∑ªÂä†Áî®Êà∑Ê∂àÊÅØÂà∞ÂéÜÂè≤
        messages.append({'role': 'user', 'content': user_input})
        
        # ÊòæÁ§∫AIÂõûÂ§ç
        ai_response_prefix = get_prompt("ai_response_prefix")
        print(f"\n{ai_response_prefix}", end='', flush=True)
        
        # Ë∞ÉÁî®AgentÂπ∂ÊµÅÂºèÊòæÁ§∫.
        # Ê≥®ÊÑè: ‰ªª‰ΩïÊù•Ëá™ agent.run() ÁöÑÂºÇÂ∏∏ (‰æãÂ¶ÇÂ∑•ÂÖ∑ÊâßË°åÈîôËØØ„ÄÅLLM APIÈîôËØØÁ≠â)
        # Âú®ËøôÈáåÈÉΩÊ≤°ÊúâË¢´ÊçïËé∑ÔºåËøôÊòØ‰∏∫‰∫ÜÁ°Æ‰øùÂΩìÂâç‰∫§‰∫íËΩÆÊ¨°ÁöÑ fail-fast Ë°å‰∏∫„ÄÇ
        # ËøôÁßçËÆæËÆ°Á¨¶ÂêàÈ°πÁõÆÁ´ãÂç≥Êö¥Èú≤ÈóÆÈ¢òÁöÑÂéüÂàô„ÄÇ
        response_text = ""
        response_messages = agent.run(messages=messages)
        
        for response in response_messages:
            response_text = typewriter_print(response, response_text)
        
        # Ê∏ÖÁêÜÂπ∂Ê∑ªÂä†ÂìçÂ∫îÂà∞ÂéÜÂè≤ - ÁâπÂà´Â§ÑÁêÜR1Ê®°ÂûãÁöÑreasoning_content
        clean_messages = []
        for msg in response_messages:
            if isinstance(msg, dict):
                # ÂàõÂª∫Ê∏ÖÁêÜÂêéÁöÑÊ∂àÊÅØÂâØÊú¨ÔºåÁßªÈô§reasoning_content
                clean_msg = {k: v for k, v in msg.items() if k != 'reasoning_content'}
                clean_messages.append(clean_msg)
            else:
                clean_messages.append(msg)
        
        messages.extend(clean_messages)
        
        # ‰øùÂ≠òÂØπËØùÂà∞ÁÆÄÂçïÂéÜÂè≤ËÆ∞ÂΩï
        memory_store['history'].append({
            'user': user_input,
            'assistant': response_text,
            'timestamp': time.time()
        })
        
        # ‰øùÊåÅÂéÜÂè≤ËÆ∞ÂΩï‰∏çË∂ÖËøá50Êù°
        if len(memory_store['history']) > 50:
            memory_store['history'] = memory_store['history'][-50:]
        
        print()  # Êç¢Ë°å


if __name__ == "__main__":
    main() 